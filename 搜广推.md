# 需要了解的算法

1. 该模型提出的动机是什么
2. 该模型训练用到的数据的特点，适合什么样的场景
3. 模型结构是什么样的？具体实现工程上的难点是什么？
4. 引入xxx为什么有效果上的提升，如注意力机制
5. 正负样本怎么选？（非常重要）

## 召回层

### ItemCF：基于物品的协同过滤（最重要的召回通道）

1. 用户对物品的索引：用户最近交互过n个物品
2. 物品对物品的索引：相似度最高的k个物品。相似度计算方式，对于两个物品，看用户重叠的数量，数量高就相似度高
3. 每次召回nk个物品，再根据兴趣分数得到100个推荐的物品

- 问题：加入重合的用户是一个小圈子，两篇笔记的相似度很低，但是因为小圈子被相互分享着看，导致算法认为这两篇笔记相似度高
  
### Swing召回通道

- 与ItemCF之间的区别：给用户设置权重，解决小圈子问题
- 如果喜欢一个物品的用户重叠大，会更认为这些用户在一个小圈子

### UserCF：基于用户的协同过滤

- 有很多跟自己兴趣相似的网友，其中某个网友赞了，我还没看，则会推荐给我
- 如何找相似网友：笔记重叠，关注的作者重叠
- 计算用户相似度公式：
- 降低热门物品的权重，因为热门物品不能反应用户的相似度。冷门物品可以很好反应用户的相似度
- 建立用户到物品的索引，用户到用户的索引

1. 给定用户ID,通过用户到用户的索引，找到相似度最高的k个用户
2. 再对每个用户，通过用户到物品的索引，找到用户最近感兴趣的物品n个
3. 召回nk个相似物品，用公式预估用户对每个物品的兴趣，选出100个物品召回

### 离散特征处理

- one-hot编码：类别数量太大不用one-hot
- embedding：pytorch的embedding层，是一个矩阵，大小为向量维度x类别数量

### 矩阵补充

- 几乎不用：仅用到物品和用户的ID，对ID进行embedding，没有利用物品和用户属性

### 近似最近邻查找

- Milvus，Faiss数据库

1. 对数据预处理，划分为多个区域，每个区域有一个代表向量
2. 看用户与这些代表向量的相似度，选取相似度最高的区域
3. 再遍历这个区域内的所有点，计算相似度。避免了遍历所有点，减少计算

### 双塔模型（超级重点，建议复现）

- 将用户ID，用户离散特征等用embedding层映射为特征向量并拼接起来，输入神经网络得到用户表征
- 类似地得到物品的表征
- 一个塔提取用户特征，另一个塔提取物品特征
- 最后再计算相似度，一般用余弦相似度
- 两个向量的余弦相似度作为兴趣的预估值
- 双塔模型属于后期融合模型，在最终输出相似度的时候才融合。前期融合模型会在提取特征后直接拼接，输入神经网络计算用于对物品的兴趣，一般用于排序

#### 双塔模型的训练

- 正样本和负样本如何选择（重点）
- Pointwise训练：对于正样本，让用于对其兴趣接近1，对于负样本，让兴趣接近-1
- Pairwise训练：正负样本分别做变换，鼓励用户对正样本兴趣大于负样本
- Listwise训练：一个正样本，多个负样本

#### 正负样本（非常重要，面试会问）

- 正样本：选择曝光且被点击的，但是这样会让热门物品更容易成为正样本
- 负样本：没有被召回的（简单负样本），被排序筛掉的（困难负样本），曝光但未被点击的
- 负样本抽样概率与热门程度呈正相关，使得热门物品不会太热，冷门物品不会太冷
- Batch内负样本：youtube论文
- 困难负样本容易被分类错误，因为被召回说明用户对它还是感兴趣的，被筛说明兴趣不够，但在计算余弦相似度的时候，容易被误判为正样本
- 一般混合简单负样本以及困难负样本
- **常见错误**：曝光没有点击的样本不能作为负样本，不能用作训练召回模型，但是可以用于训练排序模型。召回的目标是找到用户可能感兴趣的物品，不是区分比较感兴趣和很感兴趣，因此如果用曝光但没有点击的做训练召回模型是错误的。

#### 线上召回和更新

- 召回：离线存储好所有物品的特征向量，并存入向量数据库。当用户发起推荐请求时才映射用户特征，查找用户最刚兴趣的k个物品，用近似最近邻查找
- 为什么事先存储物品向量，线上计算用户向量：线上计算物品向量代价过大，用户兴趣动态变化，物品特征相对稳定，因此线上计算用户向量
- 全量更新：今天凌晨，用昨天全天的数据训练模型，训练1epoch，每天只做一次更新，发布新的用户塔神经网络和物品向量
- 增量更新：每隔几十分钟就更新。因为用户兴趣会随时发生变化。从早到晚只更新embedding层参数，不更新神经网络其他部分参数
- 今天的全量更新是基于昨天的全量模型，而不是用之前的增量更新模型。
- 为什么不只做增量更新：因为小时级数据和全天的数据有偏差。

#### 双塔模型+自监督学习

- 头部效应：少部分物品占据大部分点击
- 高点击物品表征学得好，但是长尾物品的表征学的不好
- 自监督学习训练物品塔：对两个物品特征分别做两次特征变换。对于i物品，两次变换的向量相似度尽量高。i和j物品的特征向量相似度尽量低。同理，对于j的两次变换向量相似度尽量高。
- Random Mask：随机选一些离散特征，将其遮住。会将整个类目都丢掉
- Dropout：一个物品有多个类目，dropout会丢掉类目内的某些离散特征
- 互补特征：将特征随机分组，随机mask掉部分特征得到多对物品表征，鼓励相似度尽量高
- mask一组关联的特征：例如女和美妆同时出现概率大，女和数码同时出现概率小，特征之间存在关联。
- 用于训练双塔模型的物品塔
- 双塔模型学不好低曝光物品的向量表征，因此引入自监督学习

### Deep Retrival

- 构建路径到物品的索引以及物品到路径的索引
- 如何预估用户对路径的兴趣：看视频
- beam search是什么？关键参数beam_size
- 如何做线上召回：给定用户特征，用神经网络做预估，用beam search召回一批路径。再利用索引，召回一批物品，对物品做排序，选出一个子集。user->path->items
- 如何做训练？学习神经网络参数：一个物品对应J跳路径，如果用户点击过这个物品，认为用户对物品感兴趣，那么可以设计相应损失函数
- 学习物品表征：物品与路径的相关性定义为用户对物品的兴趣 乘上 用户对路径的兴趣
- 本质：利用路径作为用户和物品的中介

### 其他召回通道

- 地理位置召回：GeoHash召回，用户对附近发生的事感兴趣
- 同城召回
- 作者召回：对作者感兴趣则会推送作者发布的笔记，新发布的笔记兴趣高
- 有交互的作者召回：对笔记感兴趣（点赞、收藏、转发），那么用户对该作者的其他笔记感兴趣
- 相似作者召回，作者到相似作者的索引。
- 缓存召回：复用前n次推荐精排的结果。精排结果大半没曝光。（可以用滑动窗口吗？）

### 曝光过滤&bloom Filter

- 如果用户看过某个物品，则不会再曝光给用户
- 利用哈希表：已曝光的物品肯定会判断正确，但是未曝光的物品会被误判，因为哈希算法有冲突
- 缺点：不支持删除物品

### 矩阵分解算法

### 基于逻辑回归的模型

### FM模型：隐向量特征交叉

### Deep crossing

### Wide&Deep
