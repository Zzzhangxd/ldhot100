# 需要了解的算法

1. 该模型提出的动机是什么
2. 该模型训练用到的数据的特点，适合什么样的场景
3. 模型结构是什么样的？具体实现工程上的难点是什么？
4. 引入xxx为什么有效果上的提升，如注意力机制
5. 正负样本怎么选？（非常重要）

## 召回层

### ItemCF：基于物品的协同过滤（最重要的召回通道）

1. 用户对物品的索引：用户最近交互过n个物品
2. 物品对物品的索引：相似度最高的k个物品。相似度计算方式，对于两个物品，看用户重叠的数量，数量高就相似度高
3. 每次召回nk个物品，再根据兴趣分数得到100个推荐的物品

- 问题：加入重合的用户是一个小圈子，两篇笔记的相似度很低，但是因为小圈子被相互分享着看，导致算法认为这两篇笔记相似度高
  
### Swing召回通道

- 与ItemCF之间的区别：给用户设置权重，解决小圈子问题
- 如果喜欢一个物品的用户重叠大，会更认为这些用户在一个小圈子

### UserCF：基于用户的协同过滤

- 有很多跟自己兴趣相似的网友，其中某个网友赞了，我还没看，则会推荐给我
- 如何找相似网友：笔记重叠，关注的作者重叠
- 计算用户相似度公式：
- 降低热门物品的权重，因为热门物品不能反应用户的相似度。冷门物品可以很好反应用户的相似度
- 建立用户到物品的索引，用户到用户的索引

1. 给定用户ID,通过用户到用户的索引，找到相似度最高的k个用户
2. 再对每个用户，通过用户到物品的索引，找到用户最近感兴趣的物品n个
3. 召回nk个相似物品，用公式预估用户对每个物品的兴趣，选出100个物品召回

### 离散特征处理

- one-hot编码：类别数量太大不用one-hot
- embedding：pytorch的embedding层，是一个矩阵，大小为向量维度x类别数量

### 矩阵补充

- 几乎不用：仅用到物品和用户的ID，对ID进行embedding，没有利用物品和用户属性

### 近似最近邻查找

- Milvus，Faiss数据库

1. 对数据预处理，划分为多个区域，每个区域有一个代表向量
2. 看用户与这些代表向量的相似度，选取相似度最高的区域
3. 再遍历这个区域内的所有点，计算相似度。避免了遍历所有点，减少计算

### 双塔模型（超级重点，建议复现）

- 将用户ID，用户离散特征等用embedding层映射为特征向量并拼接起来，输入神经网络得到用户表征
- 类似地得到物品的表征
- 一个塔提取用户特征，另一个塔提取物品特征
- 最后再计算相似度，一般用余弦相似度
- 两个向量的余弦相似度作为兴趣的预估值
- 双塔模型属于后期融合模型，在最终输出相似度的时候才融合。前期融合模型会在提取特征后直接拼接，输入神经网络计算用于对物品的兴趣，一般用于排序

#### 双塔模型的训练

- 正样本和负样本如何选择（重点）
- Pointwise训练：对于正样本，让用于对其兴趣接近1，对于负样本，让兴趣接近-1
- Pairwise训练：正负样本分别做变换，鼓励用户对正样本兴趣大于负样本
- Listwise训练：一个正样本，多个负样本

#### 正负样本（非常重要，面试会问）

- 正样本：选择曝光且被点击的，但是这样会让热门物品更容易成为正样本
- 负样本：没有被召回的（简单负样本），被排序筛掉的（困难负样本），曝光但未被点击的
- 负样本抽样概率与热门程度呈正相关，使得热门物品不会太热，冷门物品不会太冷
- Batch内负样本：youtube论文
- 困难负样本容易被分类错误，因为被召回说明用户对它还是感兴趣的，被筛说明兴趣不够，但在计算余弦相似度的时候，容易被误判为正样本
- 一般混合简单负样本以及困难负样本
- **常见错误**：曝光没有点击的样本不能作为负样本，不能用作训练召回模型，但是可以用于训练排序模型。召回的目标是找到用户可能感兴趣的物品，不是区分比较感兴趣和很感兴趣，因此如果用曝光但没有点击的做训练召回模型是错误的。

#### 线上召回和更新

- 召回：离线存储好所有物品的特征向量，并存入向量数据库。当用户发起推荐请求时才映射用户特征，查找用户最刚兴趣的k个物品，用近似最近邻查找
- 为什么事先存储物品向量，线上计算用户向量：线上计算物品向量代价过大，用户兴趣动态变化，物品特征相对稳定，因此线上计算用户向量
- 全量更新：今天凌晨，用昨天全天的数据训练模型，训练1epoch，每天只做一次更新，发布新的用户塔神经网络和物品向量
- 增量更新：每隔几十分钟就更新。因为用户兴趣会随时发生变化。从早到晚只更新embedding层参数，不更新神经网络其他部分参数
- 今天的全量更新是基于昨天的全量模型，而不是用之前的增量更新模型。
- 为什么不只做增量更新：因为小时级数据和全天的数据有偏差。

#### 双塔模型+自监督学习

- 头部效应：少部分物品占据大部分点击
- 高点击物品表征学得好，但是长尾物品的表征学的不好
- 自监督学习训练物品塔：对两个物品特征分别做两次特征变换。对于i物品，两次变换的向量相似度尽量高。i和j物品的特征向量相似度尽量低。同理，对于j的两次变换向量相似度尽量高。
- Random Mask：随机选一些离散特征，将其遮住。会将整个类目都丢掉
- Dropout：一个物品有多个类目，dropout会丢掉类目内的某些离散特征
- 互补特征：将特征随机分组，随机mask掉部分特征得到多对物品表征，鼓励相似度尽量高
- mask一组关联的特征：例如女和美妆同时出现概率大，女和数码同时出现概率小，特征之间存在关联。
- 用于训练双塔模型的物品塔
- 双塔模型学不好低曝光物品的向量表征，因此引入自监督学习

### Deep Retrival

- 构建路径到物品的索引以及物品到路径的索引
- 如何预估用户对路径的兴趣：看视频
- beam search是什么？关键参数beam_size
- 如何做线上召回：给定用户特征，用神经网络做预估，用beam search召回一批路径。再利用索引，召回一批物品，对物品做排序，选出一个子集。user->path->items
- 如何做训练？学习神经网络参数：一个物品对应J跳路径，如果用户点击过这个物品，认为用户对物品感兴趣，那么可以设计相应损失函数
- 学习物品表征：物品与路径的相关性定义为用户对物品的兴趣 乘上 用户对路径的兴趣
- 本质：利用路径作为用户和物品的中介

### TDM

- 基于索引树的方法，逐层剪枝，快速缩小候选物品范围。

### 其他召回通道

- 地理位置召回：GeoHash召回，用户对附近发生的事感兴趣
- 同城召回
- 作者召回：对作者感兴趣则会推送作者发布的笔记，新发布的笔记兴趣高
- 有交互的作者召回：对笔记感兴趣（点赞、收藏、转发），那么用户对该作者的其他笔记感兴趣
- 相似作者召回，作者到相似作者的索引。
- 缓存召回：复用前n次推荐精排的结果。精排结果大半没曝光。（可以用滑动窗口吗？）

### 曝光过滤&bloom Filter

- 如果用户看过某个物品，则不会再曝光给用户
- 利用哈希表：已曝光的物品肯定会判断正确，但是未曝光的物品会被误判，因为哈希算法有冲突
- 缺点：不支持删除物品

## 排序

1. 排序模型预估点击率、点赞率等多种分数
2. 融合这些预估分数
3. 根据分数做排序

### 多目标排序

- 训练方式：各种特征经过embedding后得到特征向量，拼接神经网络后再分别经过多个softmax层得到各种指标的预估分数，让这些预估分数拟合目标
- 训练困难：正负样本类别不平衡问题
- 预估值校准：因为负样本数量减少，那么模型的预估值往往会大于真实值，此时需要做校准

### MMoE（非常重要）

- 专家神经网络
- softmax极化：softmax输出值一个接近1，其余接近0。训练时dropout

### 预估分数的融合

### 视频播放建模

- 图文笔记排序的主要依据
- 视频排序的依据

### 排序模型的特征

- 用户画像：
- 物品画像：物品ID，发布时间，GeoHash，标题类目关键词品牌字数图片数视频清晰度标签数内容信息量
- 用户统计特征：最近30天的曝光量点击率点赞率等
- 物品统计特征：笔记最近30天的曝光量等 作者特征等
- 场景特征：用户定位、城市
- 离散特征做embedding
- 连续特征做分桶变成离散特征或者其他变换

### 粗排模型

- 粗排给几千篇笔记打分，单次推理代价必须小，预估的准确性可以不高
- 精排给几百篇笔记打分，单次推理代价很大，预估的准确性必须高
- 精排模型和双塔模型的不同（重点理解）：精排模型一般前期融合，先对所有特征进行融合。双塔模型是后期融合，不对特征融合，最后才计算向量相似度。前期融合的模型一般准确性比后期融合模型更好

#### 三塔模型

- 用户塔、物品塔、交叉塔：用户塔很大，线上只做一次推理，计算量不大。物品塔较大，离线计算。交叉塔较小，统计特征在变化，n个物品会做n次推理，因此模型要小
- 用户塔做一次推理，物品塔未命中缓冲才做推理，交叉塔做n次推理得到三个向量，再concact得到一个大特征，再softmax


### 矩阵分解算法

### 基于逻辑回归的模型

### FM模型：隐向量特征交叉

### Deep crossing

### Wide&Deep

### 特征交叉

#### FM——因式分解机

- 交叉特征的权重用矩阵分解，减少参数数量
- 使用二阶交叉特征，替代线性模型的场景

#### DeepFM

- 将FM引入Deep&Wide模型，FM代替Wide部分，使得原来Wide部分模型具有特征组合能力

#### NFM

- 对FM模型组合爆炸问题的改进，引入神经网络的方法

#### Deep&Cross

- 改进了Wide&Deep中的Wide部分，动机是增强特征交叉
- 并联两个神经网络对输出concact再softmax。可以代替召回和排序中的全连接神经网络。

### 注意力机制再推荐系统中的应用

#### AFM——引入注意力机制的FM

- 动机：FM交叉特征时，没有考虑到不同特征对结果的影响程度

#### DIN——引入注意力机制的深度学习网络（非常重要）

- 利用候选商品和历史行为商品进行相关性计算得到一个权重，这个权重代表了注意力强度
- 注意力激活单元：候选商品的embedding和用户历史行为中交互商品的embedding向量经过外积操作后，再与两个原始embeeding向量拼接，形成的embedding向量再经过全连接层交互后，形成注意力权重

#### DIEN模型——引入序列信息

- 特定用户的历史行为都是一个随时间排序的序列
- 循环神经网络的引入

#### 基于Transformer的序列化推荐模型TransAct
